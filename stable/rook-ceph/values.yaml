# Default values for rook-ceph-operator
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image:
  prefix: rook
  repository: registry.suse.com/ses/7/rook/ceph
  tag: 1.4.3
  pullPolicy: IfNotPresent

resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 256Mi

# Constrain rook-ceph-operator Deployment to nodes with a given label.
# For more info, see https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}
  # node-role.rook-ceph/operator: true

#Â Tolerations for the rook-ceph-operator to allow it to run on nodes with particular taints
tolerations:
- key: node-role.rook-ceph/operator
  operator: Equal
  value: "true"

# Delay to use in node.kubernetes.io/unreachable toleration
unreachableNodeTolerationSeconds: 5

# SUSE Enterprise Storage only supports "true": the operator should only watch for CephCluster CRDs
# in its own namespace
currentNamespaceOnly: true

## Annotations to be added to pod
annotations: {}

## LogLevel can be set to: TRACE, DEBUG, INFO, NOTICE, WARNING, ERROR or CRITICAL
logLevel: DEBUG

## If true, create & use RBAC resources
##
rbacEnable: true

## If true, create & use PSP resources
##
pspEnable: true

## Settings for whether to disable the drivers or other daemons if they are not
## needed
csi:
  enableRbdDriver: true
  enableCephfsDriver: true
  enableGrpcMetrics: true
  # (Optional) set user created priorityclassName for csi plugin pods.
  # pluginPriorityClassName: system-node-critical

  # (Optional) set user created priorityclassName for csi provisioner pods.
  # provisionerPriorityClassName: system-cluster-critical

  # Set logging level for csi containers.
  # Supported values from 0 to 5. 0 for general useful logs, 5 for trace level verbosity.
  #logLevel: 0
  # CSI CephFS plugin daemonset update strategy, supported values are OnDelete and RollingUpdate.
  # Default value is RollingUpdate.
  #rbdPluginUpdateStrategy: OnDelete
  # CSI Rbd plugin daemonset update strategy, supported values are OnDelete and RollingUpdate.
  # Default value is RollingUpdate.
  #cephFSPluginUpdateStrategy: OnDelete
  # Allow starting unsupported ceph-csi image
  allowUnsupportedVersion: false
    # (Optional) CEPH CSI RBD provisioner resource requirement list, Put here list of resource
  # requests and limits you want to apply for provisioner pod
  # csiRBDProvisionerResource: |
  #  - name : csi-provisioner
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 100m
  #      limits:
  #        memory: 256Mi
  #        cpu: 200m
  #  - name : csi-resizer
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 100m
  #      limits:
  #        memory: 256Mi
  #        cpu: 200m
  #  - name : csi-attacher
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 100m
  #      limits:
  #        memory: 256Mi
  #        cpu: 200m
  #  - name : csi-snapshotter
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 100m
  #      limits:
  #        memory: 256Mi
  #        cpu: 200m
  #  - name : csi-rbdplugin
  #    resource:
  #      requests:
  #        memory: 512Mi
  #        cpu: 250m
  #      limits:
  #        memory: 1Gi
  #        cpu: 500m
  #  - name : liveness-prometheus
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 50m
  #      limits:
  #        memory: 256Mi
  #        cpu: 100m
  # (Optional) CEPH CSI RBD plugin resource requirement list, Put here list of resource
  # requests and limits you want to apply for plugin pod
  # csiRBDPluginResource: |
  #  - name : driver-registrar
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 50m
  #      limits:
  #        memory: 256Mi
  #        cpu: 100m
  #  - name : csi-rbdplugin
  #    resource:
  #      requests:
  #        memory: 512Mi
  #        cpu: 250m
  #      limits:
  #        memory: 1Gi
  #        cpu: 500m
  #  - name : liveness-prometheus
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 50m
  #      limits:
  #        memory: 256Mi
  #        cpu: 100m
  # (Optional) CEPH CSI CephFS provisioner resource requirement list, Put here list of resource
  # requests and limits you want to apply for provisioner pod
  # csiCephFSProvisionerResource: |
  #  - name : csi-provisioner
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 100m
  #      limits:
  #        memory: 256Mi
  #        cpu: 200m
  #  - name : csi-resizer
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 100m
  #      limits:
  #        memory: 256Mi
  #        cpu: 200m
  #  - name : csi-attacher
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 100m
  #      limits:
  #        memory: 256Mi
  #        cpu: 200m
  #  - name : csi-cephfsplugin
  #    resource:
  #      requests:
  #        memory: 512Mi
  #        cpu: 250m
  #      limits:
  #        memory: 1Gi
  #        cpu: 500m
  #  - name : liveness-prometheus
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 50m
  #      limits:
  #        memory: 256Mi
  #        cpu: 100m
  # (Optional) CEPH CSI CephFS plugin resource requirement list, Put here list of resource
  # requests and limits you want to apply for plugin pod
  # csiCephFSPluginResource: |
  #  - name : driver-registrar
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 50m
  #      limits:
  #        memory: 256Mi
  #        cpu: 100m
  #  - name : csi-cephfsplugin
  #    resource:
  #      requests:
  #        memory: 512Mi
  #        cpu: 250m
  #      limits:
  #        memory: 1Gi
  #        cpu: 500m
  #  - name : liveness-prometheus
  #    resource:
  #      requests:
  #        memory: 128Mi
  #        cpu: 50m
  #      limits:
  #        memory: 256Mi
  #        cpu: 100m
  # Set provisonerTolerations and provisionerNodeAffinity for provisioner pod.
  # The CSI provisioner would be best to start on the same nodes as other ceph daemons.
  provisionerTolerations:
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: any
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: mon
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: mon-mgr
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: osd
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: mon-mgr-osd
  provisionerNodeAffinity: node-role.rook-ceph/cluster=any,mon,mon-mgr,osd,mon-mgr-osd
  # Set pluginTolerations and pluginNodeAffinity for plugin daemonset pods.
  # The CSI plugins need to be started on all the nodes where the clients need to mount the storage.
  # pluginTolerations:
  #    - key: key
  #      operator: Exists
  #      effect: NoSchedule
  # pluginNodeAffinity: key1=value1,value2; key2=value3
  # pluginNodeAffinity: node-role.rook-ceph/client=true
  #cephfsGrpcMetricsPort: 9091
  #cephfsLivenessMetricsPort: 9081
  #rbdGrpcMetricsPort: 9090
  # Enable Ceph Kernel clients on kernel < 4.17. If your kernel does not support quotas for CephFS
  # you may want to disable this setting. However, this will cause an issue during upgrades
  # with the FUSE client. See the upgrade guide: https://rook.io/docs/rook/v1.2/ceph-upgrade.html
  forceCephFSKernelClient: true
  #rbdLivenessMetricsPort: 9080
  #kubeletDirPath: /var/lib/kubelet
  #cephcsi:
    #image: registry.suse.com/ses/7/cephcsi/cephcsi:v3.1.0.0
  #registrar:
    #image: registry.suse.com/ses/7/cephcsi/csi-node-driver-registrar:v1.2.0
  #provisioner:
    #image: registry.suse.com/ses/7/cephcsi/csi-provisioner:v1.6.0
  #snapshotter:
    #image: registry.suse.com/ses/7/cephcsi/csi-snapshotter:v2.1.1
  #attacher:
    #image: registry.suse.com/ses/7/cephcsi/csi-attacher:v2.1.0
  #resizer:
    #image: registry.suse.com/ses/7/cephcsi/csi-resizer:v0.4.0

enableFlexDriver: false
enableDiscoveryDaemon: true

## if true, run rook operator on the host network
# useOperatorHostNetwork: true

## Rook Discover configuration
## tolerations: Array of tolerations in YAML format which will be added to discover deployment
## nodeAffinity: Set to labels of the node to match
discover:
  tolerations:
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: any
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: osd
  - key: node-role.rook-ceph/cluster
    operator: Equal
    value: mon-mgr-osd
  nodeAffinity: node-role.rook-ceph/cluster=any,osd,mon-mgr-osd

# In some situations SELinux relabelling breaks (times out) on large filesystems, and doesn't work with cephfs ReadWriteMany volumes (last relabel wins).
# Disable it here if you have similar issues.
# For more details see https://github.com/rook/rook/issues/2417
enableSelinuxRelabeling: true

# Writing to the hostPath is required for the Ceph mon and osd pods. Given the restricted permissions in OpenShift with SELinux,
# the pod must be running privileged in order to write to the hostPath volume, this must be set to true then.
hostpathRequiresPrivileged: false

# Disable automatic orchestration when new devices are discovered.
disableDeviceHotplug: false

# Blacklist certain disks according to the regex provided.
discoverDaemonUdev:

# imagePullSecrets option allow to pull docker images from private docker registry. Option will be passed to all service accounts.
# imagePullSecrets:
# - name: my-registry-secret

# Whether the OBC provisioner should watch on the operator namespace or not, if not the namespace of the cluster will be used
enableOBCWatchOperatorNamespace: true
